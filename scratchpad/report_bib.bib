@incollection{molnar_interpretable_2020,
	title = {Interpretable {Machine} {Learning} -- {A} {Brief} {History}, {State}-of-the-{Art} and {Challenges}},
	volume = {1323},
	url = {http://arxiv.org/abs/2010.09337},
	abstract = {We present a brief history of the field of interpretable machine learning (IML), give an overview of state-of-the-art interpretation methods, and discuss challenges. Research in IML has boomed in recent years. As young as the field is, it has over 200 years old roots in regression modeling and rule-based machine learning, starting in the 1960s. Recently, many new IML methods have been proposed, many of them model-agnostic, but also interpretation techniques specific to deep learning and tree-based ensembles. IML methods either directly analyze model components, study sensitivity to input perturbations, or analyze local or global surrogate approximations of the ML model. The field approaches a state of readiness and stability, with many methods not only proposed in research, but also implemented in open-source software. But many important challenges remain for IML, such as dealing with dependent features, causal interpretation, and uncertainty estimation, which need to be resolved for its successful application to scientific problems. A further challenge is a missing rigorous definition of interpretability, which is accepted by the community. To address the challenges and advance the field, we urge to recall our roots of interpretable, data-driven modeling in statistics and (rule-based) ML, but also to consider other areas such as sensitivity analysis, causal inference, and the social sciences.},
	urldate = {2023-10-16},
	author = {Molnar, Christoph and Casalicchio, Giuseppe and Bischl, Bernd},
	year = {2020},
	note = {arXiv:2010.09337 [cs, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	pages = {417--431},
}

@article{ribeiro_anchors:_2018,
	title = {Anchors: {High}-{Precision} {Model}-{Agnostic} {Explanations}},
	volume = {32},
	issn = {2374-3468, 2159-5399},
	shorttitle = {Anchors},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11491},
	doi = {10.1609/aaai.v32i1.11491},
	abstract = {We introduce a novel model-agnostic system that explains the behavior of complex models with high-precision rules called anchors, representing local, "sufficient" conditions for predictions. We propose an algorithm to efficiently compute these explanations for any black-box model with high-probability guarantees. We demonstrate the flexibility of anchors by explaining a myriad of different models for different domains and tasks. In a user study, we show that anchors enable users to predict how a model would behave on unseen instances with less effort and higher precision, as compared to existing linear explanations or no explanations.},
	number = {1},
	urldate = {2023-10-16},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = apr,
	year = {2018},
}

@book{molnar_interpretable_2022,
	address = {Munich, Germany},
	edition = {Second edition},
	title = {Interpretable machine learning: a guide for making black box models explainable},
	isbn = {9798411463330},
	shorttitle = {Interpretable machine learning},
	language = {eng},
	publisher = {Christoph Molnar},
	author = {Molnar, Christoph},
	year = {2022},
}

@article{gittins_dynamic_1979,
	title = {A dynamic allocation index for the discounted multiarmed bandit problem},
	volume = {66},
	issn = {0006-3444, 1464-3510},
	url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/66.3.561},
	doi = {10.1093/biomet/66.3.561},
	language = {en},
	number = {3},
	urldate = {2023-10-16},
	journal = {Biometrika},
	author = {Gittins, J. C. and Jones, D. M.},
	year = {1979},
	pages = {561--565},
}

@misc{timmiller_introduction_rl,
	title = {Introduction — {Introduction} to {Reinforcement} {Learning}},
  author = {Miller, T.},
	url = {https://gibberblot.github.io/rl-notes/intro.html},
	urldate = {2023-10-16},
}

@article{ribeiro_why_2016,
	title = {"{Why} {Should} {I} {Trust} {You}?": {Explaining} the {Predictions} of {Any} {Classifier}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {"{Why} {Should} {I} {Trust} {You}?},
	url = {https://arxiv.org/abs/1602.04938},
	doi = {10.48550/ARXIV.1602.04938},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
	urldate = {2023-10-16},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	year = {2016},
	keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
}

@article{kl_lucb_kaufmann,
author = {Kaufmann, E. and Kalyanakrishnan, S.},
year = {2013},
month = {01},
pages = {228-251},
title = {Information complexity in bandit subset selection},
volume = {30},
journal = {Journal of Machine Learning Research}
}

@Manual{anchors_R_pkg,
    title = {anchors: AnchorsOnR: High-Precision Model-Agnostic Explanations in R},
    author = {Thorben Hellweg},
    year = {2023},
    note = {R package version 0.0.1},
  }

@Manual{S7_R_pkg,
    title = {S7: An Object Oriented System Meant to Become a Successor to S3 and
S4},
    author = {Davis Vaughan and Jim Hester and Tomasz Kalinowski and Will Landau and Michael Lawrence and Martin Maechler and Luke Tierney and Hadley Wickham},
    year = {2023},
    note = {https://github.com/rconsortium/OOP-WG/,
https://rconsortium.github.io/OOP-WG/},
  }

  @Manual{purrr_R_pkg,
    title = {purrr: Functional Programming Tools},
    author = {Hadley Wickham and Lionel Henry},
    year = {2023},
    note = {https://purrr.tidyverse.org/, https://github.com/tidyverse/purrr},
  }

  @Manual{dplyr_R_pkg,
    title = {dplyr: A Grammar of Data Manipulation},
    author = {Hadley Wickham and Romain François and Lionel Henry and Kirill Müller and Davis Vaughan},
    year = {2023},
    note = {https://dplyr.tidyverse.org, https://github.com/tidyverse/dplyr},
  }

  @Book{ggplot2_R_pkg,
    author = {Hadley Wickham},
    title = {ggplot2: Elegant Graphics for Data Analysis},
    publisher = {Springer-Verlag New York},
    year = {2016},
    isbn = {978-3-319-24277-4},
    url = {https://ggplot2.tidyverse.org},
  }