---
title: "Explaining Anchors: High precision model agnostic explanations"
subtitle: "BEX6510 Foundations of Econometrics"
author: "Janith Wanniarachchi"
format: html
---

<!--
The summary and presentation slides should contain an empirical verification of the main argument, whether it is a simulation study or an application based on the real data. 
The codes related to this exercise should be submitted along with the summary and the slides. 
-->

<!--
In doing so, notice the way the paper has been organised and arguments have been developed. 
In particular, see if you can demonstrate how every claim in the paper is backed up by evidence or proof. 
In addition to introducing to you some papers of high quality not only directly or indirectly related to the material that we are learning but also related to your research area, another goal is that you see the writing style and the organisation of papers that have been published in high quality journals. Last but not least, familiarise yourself with statistical software packages/tools.
-->

<!--
 There will be additional marks when a student extends, generalises and demonstrates novelty beyond the review or a straightforward implementation or extension of the paper under review.
-->

```{r}
#| warning: false
#| message: false
#| echo: false
library(S7)
library(tidyverse) # getting a sword to cut a sandwich
library(patchwork)
library(randomForest)
library(furrr)
plan(multisession)
theme_set(theme_minimal())
```

# From building black box models to explaining black box models

Machine learning has grown from its humble start to dominating decision making processes in a variety of contexts, which has led to the need to ensure that the decisions that are automated by computational systems are unbiased, accountable, and transparent Traditionally, automated decision making was done through statistical models such as linear regression, which were inherently interpretable as models were defined based on distributional assumptions about the data and restricted the model complexity. However, these assumptions did not hold for long as began to be accumulated from many sources with higher variety, resulting in interpretable white box models failing to keep up with the desired performance. Novel machine learning models allow researchers to let the model define the functional form of the decision process. As more of our decision making processes are automated through complex models, the need to answer the question: ”how did the model arrive at this decision?” has started to gain traction over the past few years. 

Explainable AI is a research area that develops statistical and algorithmic techniques to approximate and communicate the decision process of black box models. There are many classifications of explainable AI methods, and the two most prominent classifications are centered around the model agnosticity and scope of explanations. Explainable AI methods that are capable of explaining any black box models are called model agnostic methods where the internal parameters or gradients are not utilized to explain the model. On the other hand there are model specific methods that utilize the structure of the black box model such as the weights and gradients of neural networks to provide explainations. In terms of the scope of explainations, an explaination can be capable of explaining the entire decision process of the entire all the training data instances or for a single data instance. The former would be called global explaination methods while the latter will be called local explaination methods. Example for global explaination methods would include Partial Dependence Plots and Individiual Conditional Effects while examples for local explaination methods would include LIME, SHAP and Counterfactuals.

This report will discuss and review the paper "Anchors: High precision model agnostic explanations". Section @what-are-anchors will define what anchors are and review the main arguments given by the authors of the paper, while Section @anchors-from-scratch will implement the fundamental concept of anchors in a incremental steps starting with one dimensional data and moving to higher dimensions. Finally, an overall review of the paper and the discussion on the process of reproducing the paper will be discussed in Section @review.

# What are Anchors? {#what-are-anchors}

## Theoretical definition

Anchors are a model agnostic method of explaining black box models which attempts to present the model's decision process for a single given instance. In addition to providing the decision process for the given instance, anchors provides humans the capability to predict a black box models behavior on unknown instances. This is achieved by providing a set of simple decision rules that apply to a large area of the feature space containing predictions similar to the given instance.

The authors have defined anchors as a rule or a set of predicates that satisfy the given instance and is a sufficient condition for $f(x)$ (i.e. the model output) with high probability.

A predicate is a logical condition that an observation may or may not satisfy. The simplest form of these predicates takes in the form of $\{x_1 > 2\}$. An instance with the $x_1$ feature greater than 2 would satisfy this predicate. Therefore a possible candidate for an anchor might take the form of $A = \{x_1 > 2, x_1 < 3, x_2 > 10, x_3 < 5,x_4 == 1, \dots\}$. The authors have not defined exactly how a predicate should be structured due to the wide variety of data available in the wild. Since most of our applications are on tabular data, we can assume the simplest form of orthogonal boundaries.

For an list of predicates to be an anchor it should satisfy the given instance while also maximizing two criteria,
  1. Precision
  2. Coverage

Let's dive into how each of these criteria are calculated.

### Precision

In the paper, precision is defined formally $Prec(A) = E_{D(z|A)}[1_{f(x) = f(z)}]$. Here $D$ is the perturbation distribution based on the given instance $x$. 

#### What is a perturbation distribution?

A perturbation distribution is a method of generating varied versions of the data (kind of like alternate realities of the same data). The simplest form of a pertubation distribution would be to use a multivariate normal distribution centered around the given instance.

The paper argues that it is intractable to calculate the precision directly (the term directly could be meaning analytically as it is numerically possible to approximate the expected value). Therefore a list of predicates $A$ is considered an anchor if 
$$ 
P(Prec(A) \ge \tau) \ge 1 - \delta
$$

From an implementation perspective the precision of the anchor would then be the proportion of data points from the perturbation distribution with the same class as the given instance within the boundary of the anchor.

### Coverage

The paper defines the coverage of an anchor $A$ as the probability that it applies to samples from $D$, $cov(A) = E_{D(z)}[A(z)]$. Simply put we would be calculating the proportion of samples from the perturbation distribution that satisfy the boundary of the anchor. However, this would be problematic if our perturbation distribution is dense or sparse around certain areas. 

Therefore, I would argue that picking a boundary that captures the most of the perturbation distribution is not ideal and instead it would be best to compute the coverage based on the proportion of the feature space that is covered. This idea will be covered in detail in the @def-anchors section

<!-- TODO: Explain the methodology that was used to build anchors. Including the construction of anchors using beam search, using multi armed bandits and the evaluation using user studies and simulation data -->

## Problem statement

Since a list of predicates is considered to be an anchor if it maximizes the coverage and precision, finding an anchor for a given instance can be defined as the solution to the following optimization problem

$$
\max_{A \text{ s.t. } P(Prec(A) \ge \tau) \ge 1 - \delta} cov(A)
$$

The target would then be to maximize the coverage while ensuring that the precision is above a tolerance level. However, as an extension of this approach I would argue that understanding the local neighbourhood of a given instance is more important than being able to extrapolate a model's capability on unseen instances. The decision rules with large coverage is based primarily on the perturbation distribution $D$ and therefore the choice of distribution can give flawed decision rules to end users that might only want to observe the decision rules for similar points as the given instance in the feature space.

## Methodology

The authors argue that while it is possible to generate a very large dataset and use methods such as Inductive Logic Programming to find these predicates, in high dimensional sparse datasets, the number of possible samples and predictions would be limited. Therefore they have decided to formulate the problem as a multi-armed bandit with the solution being a modified version of KL-LUCB combined with beam search to appropriately explore the possible candidates for anchors.

### Multi-armed bandit problems

Multi-armed bandit problems are a classic problem in reinforcement learning where $K$ number of choices ("arms of poker machines") are presented to an agent and each draw ("pull") from a choice gives a certain reward based on an unknown probability distribution. Some arms will give higher rewards than others. The objective is to design an agent that will maximize the average reward by the end of the sequence of draws. While trying to maximize the reward the agent will have to trade off between either exploring the available choices or exploiting the known choices that give high rewards.

There are different strategies used to solve these problems that attempt to balance the exploration-exploitation trade off such as,
1. Epsilon-greedy strategy 
2. Softmax strategy
3. Upper Confidence Bound strategy etc.

However, in this paper the authors have used a pure exploration method that tries to explore the possible choices as much as possible rather than exploiting the known arms.

### Beam search

Beam search is a heuristic search algorithm similar to Best First Search and Breadth First Search in the computer science field. The target of the algorithm is to find a possible goal state in a large graph in the shortest amount of time. Beam search achieves this by performing a breadth first search with a parameter $\beta$ limiting the breadth of the graph traversal and then ordering the visible nodes by a heuristic value to select which node to traverse to next. While traversing the graph, beam search will keep a pre determined number of nodes that could potentially be goal states and discarding ones that are not above valuable based on a heuristic.

### The algorithm

The authors have demonstrated how they developed their idea by first showcasing a greedy KL-LUCB strategy based method and then extending it to include a beam search approach.

<!-- > While the search for anchors is similar in spirit to Probabilistic Inductive Logic Programming (ILP) (De Raedt and Kersting 2008) and other rule-finding methods, one crucial difference is that we do not assume a dataset apriori - instead we have perturbation distributions and a black box model, which we can call to estimate precision and coverage bounds under D. While we could in theory generate a very large dataset and then use methods like ILP to find anchors, the number of perturbed samples and predictions from the black box model would be prohibitive, especially in high-dimensional sparse domains such as text. In order to efficiently explore the model’s behavior in the perturbation space, we turn to a multi-armed bandit formulation. -->

## Evaluation

Evaluating explainability methods quantitatively is a difficult and seemingly impossible task without utilizing a user study. Anchors is evaluated using simualted and actual users. 

### Experimental setup

<!-- We evaluate anchor explanations for complex models on a
number of tasks, primarily focusing on how they facilitate
accurate predictions by users (simulated and human) on the
behavior of the models on unseen instances. For simulated users, we use the tabular datasets previously
mentioned (adult, rcdv and lending). Each dataset is split
such that models are trained with the training set, explanations
are produced for instances in the validation set, and
evaluated on instances in the test set. For each dataset, we
train three different models: logistic regression (lr), 400 gradient
boosted trees (gb) and a multilayer perceptron with
two layers of 50 units each (nn).We generate both linear
LIME (Ribeiro, Singh, and Guestrin 2016b) and anchor explanations
for them. When simulating users, we compute coverage (what fraction
of the instances they predict after seeing explanations)
and precision (what fraction of the predictions were correct)
on the complete test set. For each dataset, model, and explanation
type, we compute these metrics for the explanation
of each instance in the validation data. Simulating when an
anchor applies is clear. It is not obvious, however, how real
users would use LIME explanations. Ideally, they should
only apply explanations to examples that are close, but it is
not clear what the distance function and the threshold for
“close” should be, or if users compute distances on demand.
Therefore, in this section, we simulate different behaviors,
and perform a study with real users in the following section. -->

### Simulated user study
The concept of simulating users is a novel and interseting concept which they have achieved by using the following procedure

### Real world user study

<!-- We ran a user study with 26 users – students who had or
were taking a machine learning course – so we could rely on
familiarity with concepts such as “model”, “prediction”, and
“cross validation”. For this study, we used the adult and rcdv
datasets, followed by a multiple-choice VQA system (Ren,
Kiros, and Zemel 2015) on two images. While the VQA
model predicts one of 1000 labels, we restrict it to the 5 most
common answers predicted on questions in D, in order to
reduce visual overload. -->

## Existing Statistical Software

Currently the anchors package has been implemented in a Java package and an R package. However, the R package simply uses the Java package under the hood. While using the package for my research purposes I found the R package to be computationally inefficient and hard to debug as the underlying computation happens in a completely different environment which is difficult to inspect. Therefore I have decided to reimplement the anchors package in a pure R package that is computationally efficient while also being simple to debug and diagnose.

# Anchors redesigned {#anchors-from-scratch}

## Implementing the foundation of anchors {#def-anchors}

Below I have defined anchors using as a collection of predicates, while a predicate is defined as the collection of a feature name, a logical operator and a constant value to compare with. 

`anchors and predicate`
```{r}
#| code-fold: show
#| code-summary: "Anchors and Predicate implementation using S7 classes"

predicate <- new_class("predicate", 
  properties = list(
    feature = class_character,
    operator = class_function,
    constant = new_union(
      class_integer, class_double, class_character
    )
  ),
  validator = function(self) {
  }
)

#| code-fold: true
anchors <- new_class("anchors",
  properties = list(
    predicates = class_vector # a vector of predicate class
  ),
  validator = function(self) {
    if(!all(sapply(self@predicates, \(x) S7_inherits(x, predicate)))) {
      return("The list of predicates should all inherit from the predicate class ")
    }
  }
)
```

In addition, there are several other functions that I have defined that work on top of anchors

An anchor should be extendable by a predicate
`extend`
```{r}
#| code-fold: show
#| code-summary: "extend method implementation"

extend <- S7::new_generic("extend", "x")
#' @param pred The predicate to extend x with
#' @return Extended anchor
S7::method(extend, anchors) <- function(x, pred) {
  x@predicates <- c(x@predicates, pred)
  return(x)
}
```

Given a dataset it tells how many of the data points are satisfied through the boundary (i.e. how many data points are inside the boundary)

`satisfies`
```{r}
#| code-fold: show
#| code-summary: "satisfies method implementation"

satisfies <- S7::new_generic("satisfies", "x")
#' @param data The dataframe to apply anchors on. Can be one instance or an entire dataset.
#' @return A logical vector indicating whether the anchors satisfies `data`
S7::method(satisfies, anchors) <- function(x, data) {
  predicate_cols <- sapply(x@predicates, \(x) x@feature)
  if (!all(predicate_cols %in% colnames(data))) {
    stop(glue::glue(
      "Predicates contain the following columns \n {predicate_cols}\n",
      "that might not be in the dataset with the following columns \n {colnames(data)}"
    ))
  }
  satis_list <- rep(TRUE, nrow(data))
  for (predicate in x@predicates) {
    result_list <- predicate@operator(data[[predicate@feature]], predicate@constant)
    satis_list <- satis_list & result_list
  }
  return(satis_list)
}
```

The way precision is defined is by collecting samples from the perturbation distribution (i.e. the varied realities of the local instance) and then selecting the ones that are within the boundary to apply the model on top of those filtered points and calculating the proportion of class labels. 
`precision`
```{r}
#| code-fold: show
#| code-summary: "precision method implementation"

precision <- S7::new_generic("precision", "x")
#' @param model a predict function that will provide the predicted labels given a dataset
#' @param dist the function that can be used to generate samples by providing an argument n. Should return a dataframe with proper column names.
#' @param n_samples the number of samples to generate from `dist` (the perturbation distribution)
#' @return named vector of proportions
S7::method(precision, anchors) <- function(x, model, dist, n_samples = 100) {
  samples <- dist(n = n_samples)
  satisfying_rows <- which(satisfies(x, samples), arr.ind = TRUE)
  if(length(satisfying_rows) == 0) {
    message("No satisfying rows found returning NULL")
    return(NULL)
  }
  samples <- samples |>
    dplyr::slice(satisfying_rows)
  preds <- model(samples)
  return(prop = as.vector(table(preds) / sum(table(preds))))
}
```

`coverage`
```{r}
#| code-fold: show
#| code-summary: "coverage method implementation"

coverage <- S7::new_generic("coverage", "x")
#' @param dist the function that can be used to generate samples by providing an argument n. Should return a dataframe with proper column names.
#' @param n_samples the number of samples to generate from `dist` (the perturbation distribution)
S7::method(coverage, anchors) <- function(x, dist, n_samples = 100) {
  samples <- dist(n = n_samples)
  return(mean(satisfies(x, samples)))
}
```

### Proposal for a new coverage method

`coverage based on feature space`
```{r}
#| code-fold: show
#| code-summary: "Coverage using area of the bounding box"

coverage_area <- S7::new_generic("coverage_area", "x")
#' @param dataset the dataset used to calculate the area upon
S7::method(coverage_area, anchors) <- function(x, dataset) {
  little_box <- dataset[satisfies(x, dataset), ]
  return(calculate_area(little_box) / calculate_area(dataset))
}

#' @description Calculates the area of the rectangular shape that encompasses the dataset by getting the range (max - min) of each column and multiplying the value across columns
calculate_area <- function(data) {
  data |> map_dbl(~max(.x, na.rm = T) - min(.x, na.rm = T)) |> prod()
}
```

Instead of calculating the coverage based on the perturbation distribution there is also the possibility of calculating the coverage based on the size of the bounding box compared to the entire dataset.

## Brute force approach on one dimensional data

Now that we have a rough idea of what anchors are, we will begin by reproducing the concepts given in the paper. Instead of simply reproducing the final algorithm mentioned in the paper we will be emulating the thought process of constructing the final solution by developing the idea of anchors with the simplest case to more advanced complex scenarios. Throughout this tutorial we will be using random forest models as an example black box models for simplicity and ease of implementation.

Let's explore this idea with a simple one dimensional example. First we generate data in the range of $[0,1]$ and assign a binary class based on the following criteria. This dataset will be considered as the population from which we would collect a sample dataset to consider as the observed data. The observed data will then be segmented into training and testing data to develop black box models.

### Generating data

```{r}
#| code-fold: true
#| code-summary: "Code to generate one dimensional data"

# get the population x
pop_x <- seq(0,1,by = 0.01)
# assign a class
outcome <- ifelse(sin(15 * pop_x) > 0, "Plus", "Minus")
pop_data <- tibble(x = pop_x, class = factor(outcome))
# visualize population
pop_plot <- ggplot(pop_data, aes(x = x,color = class, y =0)) +
  geom_point() +
  labs(title = "Population data for 1 dimension")

# sample half of it
set.seed(100)
sample_data <- pop_data |> slice_sample(prop = 0.5, by = class)
obs_plot <- ggplot(sample_data, aes(x = x,color = class, y =0)) +
  geom_point() +
  labs(title = "Observed sample data")

# create a training and testing set
set.seed(200)
train_df <- sample_data |> slice_sample(prop = 0.7, by = class)
test_df <- sample_data |> slice_sample(prop = 0.3, by = class)
train_plot <- ggplot(train_df, aes(x = x,color = class, y =0)) +
  geom_point() +
  labs(title = "Training data")

pop_plot / obs_plot / train_plot
```

```{r}
#| code-fold: true
#| code-summary: "Code to fit random forest model"

# fit a randomforest model
rfmodel <- randomForest(class ~ x, data = train_df, ntree = 10)
rfmodel
```

Now how do we explain the decision process of this black box model using anchors. While it is possible to explain the decision process of a randomforest model with 10 trees fitted on a 1-dimensional data, we want to start explaining the process of anchors with a simpler case of one dimensional data.

### Demonstration on a single point
Let us first pick a data point in the dataset. Ideally a point in a border would be best to illustrate the idea. Hence we will pick the first observation as follows.

```{r}
#| code-fold: true

local_instance <- 1
ggplot(
  train_df[-local_instance, ],
  aes(x = x,color = class, y = 0)
) +
  geom_point() +
  geom_point(data = train_df[local_instance, ], color = "black", size = 2.5) +
  geom_label(data = train_df[local_instance,], label = "here", color = "black", nudge_y = 0.005) +
  lims(y = c(-0.05, 0.05))
```

For a one dimensional example, a boundary region would be defined by two values, a value left to the given value and a value right to the given value.

```{r}
#| code-fold: true
#| code-summary: "Code to generate cutpoints"

x_vals <- train_df[-local_instance,][["x"]] |> sort()
x_cutpoints <- purrr::map2_dbl(x_vals[-length(x_vals)], x_vals[-1], function(x, x_1) {
  return(mean(c(x, x_1)))
})
x_grid <- expand.grid(
  x_cutpoints[x_cutpoints < train_df[local_instance,]$x],
  x_cutpoints[x_cutpoints > train_df[local_instance,]$x]
) |> rename(a = Var1, b = Var2)
```

All possible cutpoints

```{r}
#| echo: false
#| eval: false
ggplot(
  train_df[-local_instance, ],
  aes(x = x,color = class, y = 0)
) +
  geom_point() +
  geom_point(data = train_df[local_instance, ], color = "black", size = 2.5) +
  geom_label(data = train_df[local_instance,], label = "here", color = "black", nudge_y = 0.005) +
  lims(y = c(-0.05, 0.05)) +
  geom_vline(data = x_grid, aes(xintercept = a), color = "purple", linetype = "dashed", alpha = 0.4) +
  geom_vline(data = x_grid, aes(xintercept = b), color = "gray", linetype = "dashed", alpha = 0.8)
```

Let's visualize how a few bounding boxes should look like

```{r}
#| code-fold: true

ggplot(
  train_df[-local_instance, ],
  aes(x = x, y = 0)
) +
  geom_point() +
  geom_point(data = train_df[local_instance, ], color = "black", size = 2.5) +
  geom_label(data = train_df[local_instance,], label = "here", color = "black", nudge_y = 0.005) +
  lims(y = c(-0.05, 0.05)) +
  geom_rect(
    data = x_grid |>
      slice_sample(n = 3) |>
      mutate(id = row_number()),
    aes(xmin = a, xmax = b, ymin = -0.05,ymax = 0.05, color = factor(id)),
    linetype = "dashed",
    fill = "transparent",
    inherit.aes = FALSE
  )
```

For a one dimensional example the simplest solution would be to apply a brute force approach and calculate the precision and coverage for all the possible bounding boxes. In order to calculate the precision and coverage we would need to define the model function $f$ and the pertubation distribution $D$. For this specific case, the perturbation distribution would be $[0.01,0.02, \dots, 0.99, 1.00]$

```{r}
model_func <- function(data_samples) {
  return(predict(rfmodel, data_samples))
}

dist_func <- function(n) data.frame(x = seq(0,1,by = 0.01))
```


### Brute force approach

```{r}
#| code-fold: true
#| code-summary: "Code for brute force approach"

res <- x_grid |> apply(1, function(row) {
  bound <- anchors(c(
    predicate(feature = "x",operator = `>`,constant = row["a"]),
    predicate(feature = "x",operator = `<`,constant = row["b"])
  ))
  cover <- coverage(bound, dist_func, n_samples = 500)
  cover_area <- coverage_area(bound, train_df |> select(x))
  prec <- precision(bound, model_func, dist_func, n_samples = 500)
  return(list(cover = cover, cover_area = cover_area, prec = prec))
})
```


```{r}
#| code-fold: true
#| code-summary: "Visualizing the relationship between precision and coverage"

res_df <- res |>
  map_dfr(~ tibble(
      cover = .x$cover,
      cover_area = .x$cover_area,
      prec_1 = .x$prec[1],
      prec_2 = .x$prec[2])
  )
res_df |>
  ggplot(aes(x = prec_1, y = cover_area)) +
  geom_point() +
  geom_vline(xintercept = 0.8) +
  labs(title = "Precision vs Coverage for all possible bounding boxes in 1 dimension")
```

Let us visualize the bounding box with the highest precision.

```{r}
max_prec_bound <- bind_cols(x_grid, res_df) |>
  slice_max(prec_1)

max_prec_train_plot <- ggplot(
  train_df[-local_instance, ],
  aes(x = x,color = class, y = 0)
) +
  geom_point() +
  geom_point(data = train_df[local_instance, ], color = "black", size = 2.5) +
  geom_label(data = train_df[local_instance,], label = "here", color = "black", nudge_y = 0.005) +
  lims(y = c(-0.05, 0.05)) +
  geom_rect(
    data = max_prec_bound,
    aes(xmin = a, xmax = b, ymin = -0.05,ymax = 0.05),
    linetype = "dashed",
    color = "purple",
    fill = "transparent",
    inherit.aes = FALSE,
    show.legend = FALSE
  ) +
  labs(title = glue::glue(
    "Precision: {round(max_prec_bound$prec_1,2)}",
    " Coverage = {round(max_prec_bound$cover,2)}"
    ),
    subtitle = glue::glue("IF x > {max_prec_bound$a} AND x < {max_prec_bound$b}"),
    caption = "Plotted points are training data"
  )
```

```{r}
max_prec_pop_plot <- ggplot(
  pop_data,
  aes(x = x,color = class, y = 0)
) +
  geom_point() +
  geom_point(data = train_df[local_instance, ], color = "black", size = 2.5) +
  geom_label(data = train_df[local_instance,], label = "here", color = "black", nudge_y = 0.005) +
  lims(y = c(-0.05, 0.05)) +
  geom_rect(
    data = max_prec_bound,
    aes(xmin = a, xmax = b, ymin = -0.05,ymax = 0.05),
    linetype = "dashed",
    color = "purple",
    fill = "transparent",
    inherit.aes = FALSE,
    show.legend = FALSE
  ) +
  labs(title = glue::glue(
    "Precision: {round(max_prec_bound$prec_1,2)}",
    " Coverage = {round(max_prec_bound$cover,2)}"
    ),
    subtitle = glue::glue("IF x > {max_prec_bound$a} AND x < {max_prec_bound$b}"),
    caption = "Plotted points are population points")

max_prec_pop_plot / max_prec_train_plot 
```

The most optimal bounding box

```{r}
optimal_bound <- bind_cols(x_grid, res_df) |>
  arrange(desc(cover), desc(prec_1)) |>
  filter(prec_1 > 0.8) |> 
  slice(1)

optimal_bound_train_plot <- ggplot(
  train_df[-local_instance, ],
  aes(x = x,color = class, y = 0)
) +
  geom_point() +
  geom_point(data = train_df[local_instance, ], color = "black", size = 2.5) +
  geom_label(data = train_df[local_instance,], label = "here", color = "black", nudge_y = 0.005) +
  lims(y = c(-0.05, 0.05)) +
  geom_rect(
    data = optimal_bound,
    aes(xmin = a, xmax = b, ymin = -0.05,ymax = 0.05),
    linetype = "dashed",
    color = "purple",
    fill = "transparent",
    inherit.aes = FALSE,
    show.legend = FALSE
  ) +
  labs(title = glue::glue(
    "Precision: {round(optimal_bound$prec_1,2)}",
    " Coverage = {round(optimal_bound$cover,2)}"
    ),
    subtitle = glue::glue("IF x > {optimal_bound$a} AND x < {optimal_bound$b}"),
    caption = "Plotted points are training data points"
  )
```

```{r}
optimal_bound_pop_plot <- ggplot(
  pop_data,
  aes(x = x,color = class, y = 0)
) + 
  geom_point() +
  geom_point(data = train_df[local_instance, ], color = "black", size = 2.5) +
  geom_label(data = train_df[local_instance,], label = "here", color = "black", nudge_y = 0.005) +
  lims(y = c(-0.05, 0.05)) +
  geom_rect(
    data = optimal_bound,
    aes(xmin = a, xmax = b, ymin = -0.05,ymax = 0.05),
    linetype = "dashed",
    color = "purple",
    fill = "transparent",
    inherit.aes = FALSE,
    show.legend = FALSE
  ) +
  labs(title = glue::glue(
    "Precision: {round(optimal_bound$prec_1,2)}",
    " Coverage = {round(optimal_bound$cover,2)}"
    ),
    subtitle = glue::glue("IF x > {optimal_bound$a} AND x < {optimal_bound$b}"),
    caption = "Plotted points are population points"
  )

optimal_bound_pop_plot / optimal_bound_train_plot
```

### Redemonstration for robustness

Now let's attempt the above for a different point and see how the bounding box changes.

```{r}
local_instance <- 6

ggplot(
  train_df[-local_instance, ],
  aes(x = x,color = class, y = 0)
) +
  geom_point() +
  geom_point(data = train_df[local_instance, ], color = "black", size = 2.5) +
  geom_label(data = train_df[local_instance,], label = "here", color = "black", nudge_y = 0.005) +
  lims(y = c(-0.05, 0.05))
```

We will need to generate a new set of possible bounding boxes that surround the new instance of interest. After generating these bounding boxes we can calculate the precision and accuracy for each of these bounding boxes and obtain the bounding box with highest precision and the most optimal bounding box.


```{r}
x_vals <- train_df[-local_instance,][["x"]] |> sort()
x_cutpoints <- purrr::map2_dbl(x_vals[-length(x_vals)], x_vals[-1], function(x, x_1) {
  return(mean(c(x, x_1)))
})
x_grid <- expand.grid(
  x_cutpoints[x_cutpoints < train_df[local_instance,]$x],
  x_cutpoints[x_cutpoints > train_df[local_instance,]$x]
) |> rename(a = Var1, b = Var2)

res <- x_grid |> apply(1, function(row) {
  bound <- anchors(c(
    predicate(feature = "x",operator = `>`,constant = row["a"]),
    predicate(feature = "x",operator = `<`,constant = row["b"])
  ))
  cover <- coverage(bound, dist_func, n_samples = 500)
  prec <- precision(bound, model_func, dist_func, n_samples = 500)
  return(list(cover = cover, prec = prec))
})

res_df <- res |>
  map_dfr(~ tibble(
      cover = .x$cover,
      prec_1 = .x$prec[1],
      prec_2 = .x$prec[2])
  )

res_df |>
  ggplot(aes(x = prec_1, y = cover)) +
  geom_point() +
  geom_vline(xintercept = 0.8) +
  labs(title = "Precision vs Coverage for all possible bounding boxes in 1 dimension")
```


```{r}
max_prec_bound <- bind_cols(x_grid, res_df) |>
  slice_max(prec_1) |>
  # since there are more than one box with highest precision we will be selecting the bounding box we will be selecting a bounding box at random
  slice_sample(n = 1)
max_prec_train_plot_alt <- ggplot(train_df[-local_instance, ], aes(x = x,color = class, y = 0)) + geom_point() + geom_point(data = train_df[local_instance, ], color = "black", size = 2.5) +
  geom_label(data = train_df[local_instance,], label = "here", color = "black", nudge_y = 0.005) +
  lims(y = c(-0.05, 0.05)) +
  geom_rect(
    data = max_prec_bound,
    aes(xmin = a, xmax = b, ymin = -0.05,ymax = 0.05),
    linetype = "dashed",
    color = "purple",
    fill = "transparent",
    inherit.aes = FALSE,
    show.legend = FALSE
  ) + labs(title = glue::glue("Precision: {round(max_prec_bound$prec_1,2)}", " Coverage = {round(max_prec_bound$cover,2)}"), subtitle = glue::glue("IF x > {max_prec_bound$a} AND x < {max_prec_bound$b}"), caption = "Plotted points are training data")

max_prec_pop_plot_alt <- ggplot(pop_data, aes(x = x,color = class, y = 0)) + geom_point() + geom_point(data = train_df[local_instance, ], color = "black", size = 2.5) +
  geom_label(data = train_df[local_instance,], label = "here", color = "black", nudge_y = 0.005) +
  lims(y = c(-0.05, 0.05)) +
  geom_rect(
    data = max_prec_bound,
    aes(xmin = a, xmax = b, ymin = -0.05,ymax = 0.05),
    linetype = "dashed",
    color = "purple",
    fill = "transparent",
    inherit.aes = FALSE,
    show.legend = FALSE
  ) +
  labs(title = glue::glue("Precision: {round(max_prec_bound$prec_1,2)}", " Coverage = {round(max_prec_bound$cover,2)}"), subtitle = glue::glue("IF x > {max_prec_bound$a} AND x < {max_prec_bound$b}"), caption = "Plotted points are population points")

optimal_bound <- bind_cols(x_grid, res_df) |>
  arrange(desc(cover), desc(prec_1)) |>
  filter(prec_1 > 0.8) |> 
  slice(1)

optimal_bound_train_plot_alt <- ggplot(train_df[-local_instance, ], aes(x = x,color = class, y = 0)) + geom_point() + geom_point(data = train_df[local_instance, ], color = "black", size = 2.5) +
  geom_label(data = train_df[local_instance,], label = "here", color = "black", nudge_y = 0.005) +
  lims(y = c(-0.05, 0.05)) +
  geom_rect(
    data = optimal_bound,
    aes(xmin = a, xmax = b, ymin = -0.05,ymax = 0.05),
    linetype = "dashed",
    color = "purple",
    fill = "transparent",
    inherit.aes = FALSE,
    show.legend = FALSE
  ) +
  labs(title = glue::glue("Precision: {round(optimal_bound$prec_1,2)}", " Coverage = {round(optimal_bound$cover,2)}"), subtitle = glue::glue("IF x > {optimal_bound$a} AND x < {optimal_bound$b}"), caption = "Plotted points are training data points")

optimal_bound_pop_plot_alt <- ggplot(pop_data, aes(x = x,color = class, y = 0)) + geom_point() + geom_point(data = train_df[local_instance, ], color = "black", size = 2.5) +
  geom_label(data = train_df[local_instance,], label = "here", color = "black", nudge_y = 0.005) +
  lims(y = c(-0.05, 0.05)) +
  geom_rect(
    data = optimal_bound,
    aes(xmin = a, xmax = b, ymin = -0.05,ymax = 0.05),
    linetype = "dashed",
    color = "purple",
    fill = "transparent",
    inherit.aes = FALSE,
    show.legend = FALSE
  ) +
  labs(title = glue::glue("Precision: {round(optimal_bound$prec_1,2)}", " Coverage = {round(optimal_bound$cover,2)}"), subtitle = glue::glue("IF x > {optimal_bound$a} AND x < {optimal_bound$b}"), caption = "Plotted points are population points")

(max_prec_train_plot / optimal_bound_train_plot) | (max_prec_train_plot_alt / optimal_bound_train_plot_alt)
```

## Sequential Greedy Approach in two dimensions

We are going to use the previous brute force approach sequentially across dimensions, and as an example we are going to use an example two dimensional dataset.

Now let's try to perform this for two dimensions.

```{r}
w <- read_csv("wiggly.csv",
              col_select = -1,
              col_types = cols(
                x = col_double(),
                y = col_double(),
                class = col_double())) |> 
  mutate(class = factor(ifelse(class == 3, "Positive", "Negative")))

observed_2dim_plot <- w |> 
  ggplot(aes(x = x,y = y,color = class)) +
  geom_point() +
  labs(title = "Observed data for 2 dimensions") +
  coord_equal()
```

```{r}
# sample train data
set.seed(69420)
train_indices <- sample(nrow(w), round(nrow(w) * 0.8))
train_2dim_plot <- w[train_indices, ] |> 
  ggplot(aes(x = x,y = y,color = class)) +
  geom_point() +
  labs(title = "Training data") + 
  coord_equal()

observed_2dim_plot | train_2dim_plot
```

```{r}
train_df <- w[train_indices, ] |> mutate(id = row_number())
library(randomForest)
rfmodel <- randomForest(class ~ x + y, data = train_df, ntree = 5)
rfmodel
```

When selecting instances we will select instances that the model is having difficulty predicting. These points are most likely situated in the boundary area and therefore would be ideal candidate in exploring how anchors work.

```{r}
# select instance
prob_matrix <- predict(rfmodel, newdata = train_df, type = "prob") |> 
  as.data.frame() |>
  mutate(id = row_number())
local_instance <- prob_matrix[prob_matrix$Negative == 0.4, "id"]
local_instance <- local_instance[1]
```


```{r}
train_df[-local_instance, ] |>
  ggplot(aes(x = x,y = y,color = class)) +
  geom_point() +
  geom_point(data = train_df[local_instance, ], size = 5, color = "black") +
  geom_label(data = train_df[local_instance, ], label = "here", color = "black", nudge_y = 0.05)
```

```{r}
# generate cutpoints
x_vals <- train_df[-local_instance,][["x"]] |> sort()
x_cutpoints <- purrr::map2_dbl(x_vals[-length(x_vals)], x_vals[-1], function(x, x_1) {
  return(mean(c(x, x_1)))
})
x_grid <- expand.grid(
  x_cutpoints[x_cutpoints < train_df[local_instance,]$x],
  x_cutpoints[x_cutpoints > train_df[local_instance,]$x]
) |> 
  rename(L = Var1, U = Var2)|> 
  as_tibble() |>
  arrange(desc(L), U)

y_vals <- train_df[-local_instance,][["y"]] |> sort()
y_cutpoints <- purrr::map2_dbl(y_vals[-length(y_vals)], y_vals[-1], function(x, x_1) {
  return(mean(c(x, x_1)))
})
y_grid <- expand.grid(
  y_cutpoints[y_cutpoints < train_df[local_instance,]$y],
  y_cutpoints[y_cutpoints > train_df[local_instance,]$y]
) |> 
  rename(L = Var1, U = Var2) |> 
  as_tibble() |>
  arrange(desc(L), U)
```

```{r}
pertub_func <- function(n) {
  mulgar::rmvn(n = n, 
               p = 2,
               mn = train_df[local_instance, c("x", "y")] |> 
                 unlist(),
               vc = cov(train_df[,c("x", "y")])
  ) |>
    as.data.frame() |>
    rename(x = x1, y = x2)
}

model_func <- function(data_samples) {
  suppressPackageStartupMessages(library(randomForest)) # for future package
  return(predict(rfmodel, data_samples))
}

set.seed(123)
samples <- pertub_func(n = 10000)
dist_func <- function(n) samples[1:n, ]
```

We are going to brute force the two dimensional approach sequentially

```{r}
#| eval: true
#| warning: false
#| message: false

# define final anchor to be null
final_anchor <- NULL
dimensions <- list("x" = x_grid,"y" = y_grid) # variable names as names
results <- imap(dimensions, function(bounds, var_name){
  dim_results <- future_map_dfr(seq_len(nrow(bounds)), function(i) {
    row <- bounds[i, ]
    lower_bound_pred <- predicate(feature = var_name, operator = `>`, constant = row[["L"]])
    upper_bound_pred <- predicate(feature = var_name, operator = `<`, constant = row[["U"]])
    if(is.null(final_anchor)) {
      bound <- anchors(c(lower_bound_pred, upper_bound_pred))  
    } else {
      bound <- final_anchor |>
        extend(lower_bound_pred) |>
        extend(upper_bound_pred)
    }
    cover <- coverage(bound, dist_func, n_samples = 10000)
    prec <- precision(bound, model_func, dist_func, n_samples = 10000)
    bind_cols(row, tibble(cover = cover, precision_1 = prec[1], precision_2 = prec[2]))
  }, .options = furrr_options(globals = c("satisfies", "predicate", "anchors", "coverage", "precision", "extend", "final_anchor", "dist_func", "model_func", "dimensions", "samples", "rfmodel", "bind_cols")))
  max_prec <- dim_results |> slice_max(precision_1) |> head(1)
  best_lower_bound <- predicate(feature = var_name, operator = `>`, constant = max_prec[["L"]])
  best_upper_bound <- predicate(feature = var_name, operator = `<`, constant = max_prec[["U"]])
  if(is.null(final_anchor)) {
    final_anchor <<- anchors(c(best_lower_bound, best_upper_bound))  
  } else {
    final_anchor <<- final_anchor |>
        extend(best_lower_bound) |>
        extend(best_upper_bound)
  }
  list(res = dim_results, lb = best_lower_bound, ub = best_upper_bound)
})
```


```{r}
x_cov_prec_plot <- results$x$res |> ggplot(aes(x = precision_1, y = cover)) + geom_point()
y_cov_prec_plot <- results$y$res |> ggplot(aes(x = precision_1, y = cover)) + geom_point()
x_cov_prec_plot | y_cov_prec_plot
```


```{r}
#| eval: false

train_df[-local_instance, ] |>
  ggplot(aes(x = x,y = y,color = class)) +
  geom_point() +
  geom_point(data = train_df[local_instance, ], size = 1, color = "black") +
  geom_label(data = train_df[local_instance, ], label = "here", color = "black", nudge_y = 0.05) +
  geom_rect(inherit.aes = F, 
            data = tibble(x_lb = results$x$lb@constant,
                          y_lb = results$y$lb@constant,
                          x_ub = results$x$ub@constant,
                          y_ub = results$y$ub@constant),
            aes(xmin = x_lb, xmax = x_ub, ymin = y_lb, ymax = y_ub), fill = "transparent", color = "black")
```

## Simple Multi Armed Bandits solution using Upper Confidence Bounds in two dimensions

The sequential greedy method does seem to be generating good boundaries.
Let us try modeling this as a multiarmed bandit problem.

the actions are to increase the lower / upper bound of each dimension.

```{r}
# TODO we can actually model the actions as changing the bounding box in a given direction
envir <- list(
  x_lb = x_grid$L |> unique(),
  x_ub = x_grid$U |> unique(),
  y_lb = y_grid$L |> unique(),
  y_ub = y_grid$U |> unique()
)
actions <- c("x_lb", "x_ub", "y_lb", "y_ub")
```

```{r}
get_reward <- function(x_lb_ind, x_ub_ind, y_lb_ind, y_ub_ind, dist_func, model_func, class_ind = 1) {
  bound <- anchors(c(
    predicate(feature = "x",operator = `>`,constant = envir$x_lb[x_lb_ind]),
    predicate(feature = "x",operator = `<`,constant = envir$x_ub[x_ub_ind]),
    predicate(feature = "y",operator = `>`,constant = envir$y_lb[y_lb_ind]),
    predicate(feature = "y",operator = `<`,constant = envir$y_ub[y_ub_ind])
  ))
  # if(!satisfies(bound, train_df[local_instance, ])) {
  #   print(c(x_lb_ind, x_ub_ind, y_lb_ind, y_ub_ind))
  #   return(-9999) # penalty
  # }
  cover <- coverage(bound, dist_func, n_samples = 10000)
  prec <- precision(bound, model_func, dist_func, n_samples = 10000)
  if(is.null(prec)) return(-9999) # penalty
  if(prec[class_ind] < 0.6) {
    return(-9999) # penalty for wrong direction
  } 
  # return(2 * prec[class_ind] + 0.5 * cover) # to put more weight on precision
  return(prec[class_ind])
}

select_action <- function(Q, N, n_game) {
  rewards <- map_dbl(actions, function(a){
    Q[[a]] + sqrt((2 * log(n_game)) / N[[a]])
  })
  if(sum(rewards == max(rewards)) > 1) {
    max_reward <- sample(which(rewards == max(rewards)), 1)
  } else {
    max_reward <- which.max(rewards)  
  }
  return(actions[max_reward])
}
```

```{r}
n_games <- 10
n_epochs <- 100
```

```{r}
#| eval: false

for(game in seq_len(n_games)) {
  x_ub_ind <- 1
  x_lb_ind <- 1
  y_ub_ind <- 1
  y_lb_ind <- 1
  N <- list(x_ub = 1, y_ub = 1, x_lb = 1, y_lb = 1)
  Q <- list(x_ub = 0, y_ub = 0, x_lb = 0, y_lb = 0)
  for(epoch in seq_len(n_epochs)) {
    action <- select_action(Q, N, game)
    if(action == "x_ub") x_ub_ind <- ifelse(x_ub_ind == length(envir$x_ub), x_ub_ind, x_ub_ind + 1)
    if(action == "y_ub") y_ub_ind <- ifelse(y_ub_ind == length(envir$y_ub), y_ub_ind, y_ub_ind + 1)
    if(action == "x_lb") x_lb_ind <- ifelse(x_lb_ind == length(envir$x_lb), x_lb_ind, x_lb_ind + 1)
    if(action == "y_lb") y_lb_ind <- ifelse(y_lb_ind == length(envir$y_lb), y_lb_ind, y_lb_ind + 1)
    reward <- get_reward(x_lb_ind, x_ub_ind, y_lb_ind, y_ub_ind, dist_func, model_func)
    if(reward < 0) {
      # if a penalty was received we undo the action and go on with the rest
      if(action == "x_ub") x_ub_ind <- ifelse(x_ub_ind == 1, x_ub_ind, x_ub_ind - 1)
      if(action == "y_ub") y_ub_ind <- ifelse(y_ub_ind == 1, y_ub_ind, y_ub_ind - 1)
      if(action == "x_lb") x_lb_ind <- ifelse(x_lb_ind == 1, x_lb_ind, x_lb_ind - 1)
      if(action == "y_lb") y_lb_ind <- ifelse(y_lb_ind == 1, y_lb_ind, y_lb_ind - 1)
      next
    }
    N[[action]] <- N[[action]] + 1
    Q[[action]] <- Q[[action]] + ((reward - Q[[action]]) / N[[action]])
    if(epoch %% 10 == 0) {
      state_plot <- train_df[-local_instance, ] |>
        ggplot(aes(x = x,y = y,color = class)) +
        geom_point() +
        geom_point(data = train_df[local_instance, ], size = 1, color = "black") +
        geom_label(data = train_df[local_instance, ], label = "here", color = "black", nudge_y = 0.05) +
        geom_rect(inherit.aes = F,
                  data = tibble(x_lb = envir$x_lb[x_lb_ind],
                                y_lb = envir$y_lb[y_lb_ind],
                                x_ub = envir$x_ub[x_ub_ind],
                                y_ub = envir$y_ub[y_ub_ind]),
                  aes(xmin = x_lb, xmax = x_ub, ymin = y_lb, ymax = y_ub), fill = "transparent", color = "black") +
        labs(title = glue::glue("Game: {game}, round: {epoch}, reward: {reward}"))
      ggsave(plot = state_plot,
             filename = here::here("scratchpad/3_state_plot_dump/", glue::glue("{game}_{epoch}.png")),
             device = "png", bg = "white", width = 11, height = 8, units = "in")
    }
  }
}
```


## Redemonstration of both approaches for robustness

We will apply this instance to a second point to observe robustness using the sequential greedy approach.

```{r}
# train_df |> mutate(id = row_number()) |> ggplot(aes(x = x,y = y, color = class)) + geom_point() + geom_label(aes(label = id), size =2)
local_instance <- 139
train_df[-local_instance, ] |>
  ggplot(aes(x = x,y = y,color = class)) +
  geom_point() +
  geom_point(data = train_df[local_instance, ], size = 5, color = "black") +
  geom_label(data = train_df[local_instance, ], label = "here", color = "black", nudge_y = 0.05)
```


```{r}
x_vals <- train_df[-local_instance,][["x"]] |> sort()
x_cutpoints <- purrr::map2_dbl(x_vals[-length(x_vals)], x_vals[-1], function(x, x_1) {
  return(mean(c(x, x_1)))
})
x_grid <- expand.grid(
  x_cutpoints[x_cutpoints < train_df[local_instance,]$x],
  x_cutpoints[x_cutpoints > train_df[local_instance,]$x]
) |> 
  rename(L = Var1, U = Var2)|> 
  as_tibble() |>
  arrange(desc(L), U)

y_vals <- train_df[-local_instance,][["y"]] |> sort()
y_cutpoints <- purrr::map2_dbl(y_vals[-length(y_vals)], y_vals[-1], function(x, x_1) {
  return(mean(c(x, x_1)))
})
y_grid <- expand.grid(
  y_cutpoints[y_cutpoints < train_df[local_instance,]$y],
  y_cutpoints[y_cutpoints > train_df[local_instance,]$y]
) |> 
  rename(L = Var1, U = Var2) |> 
  as_tibble() |>
  arrange(desc(L), U)
```

```{r}
# define final anchor to be null
final_anchor <- NULL
dimensions <- list("x" = x_grid,"y" = y_grid) # variable names as names
results <- imap(dimensions, function(bounds, var_name){
  dim_results <- future_map_dfr(seq_len(nrow(bounds)), function(i) {
    row <- bounds[i, ]
    lower_bound_pred <- predicate(feature = var_name, operator = `>`, constant = row[["L"]])
    upper_bound_pred <- predicate(feature = var_name, operator = `<`, constant = row[["U"]])
    if(is.null(final_anchor)) {
      bound <- anchors(c(lower_bound_pred, upper_bound_pred))  
    } else {
      bound <- final_anchor |>
        extend(lower_bound_pred) |>
        extend(upper_bound_pred)
    }
    cover <- coverage(bound, dist_func, n_samples = 10000)
    prec <- precision(bound, model_func, dist_func, n_samples = 10000)
    bind_cols(row, tibble(cover = cover, precision_1 = prec[1], precision_2 = prec[2]))
  }, .options = furrr_options(globals = c("satisfies", "predicate", "anchors", "coverage", "precision", "extend", "final_anchor", "dist_func", "model_func", "dimensions", "samples", "rfmodel", "bind_cols")))
  max_prec <- dim_results |> slice_max(precision_2) |> head(1)
  best_lower_bound <- predicate(feature = var_name, operator = `>`, constant = max_prec[["L"]])
  best_upper_bound <- predicate(feature = var_name, operator = `<`, constant = max_prec[["U"]])
  if(is.null(final_anchor)) {
    final_anchor <<- anchors(c(best_lower_bound, best_upper_bound))  
  } else {
    final_anchor <<- final_anchor |>
        extend(best_lower_bound) |>
        extend(best_upper_bound)
  }
  list(res = dim_results, lb = best_lower_bound, ub = best_upper_bound)
})
```

```{r}
train_df[-local_instance, ] |>
  ggplot(aes(x = x,y = y,color = class)) +
  geom_point() +
  geom_point(data = train_df[local_instance, ], size = 1, color = "black") +
  geom_label(data = train_df[local_instance, ], label = "here", color = "black", nudge_y = 0.05) +
  geom_rect(inherit.aes = F, 
            data = tibble(x_lb = results$x$lb@constant,
                          y_lb = results$y$lb@constant,
                          x_ub = results$x$ub@constant,
                          y_ub = results$y$ub@constant),
            aes(xmin = x_lb, xmax = x_ub, ymin = y_lb, ymax = y_ub), fill = "transparent", color = "black")
```

# Discussion and Review {#review}

## Remarks on the original paper

The concept of anchors is an interesting approach to the topic of providing local explanations of black box models. Using human comprehensible decision rules defined as bounding boxes (a list of predicates) to explain the model's decision process around the local instance makes it easier for human's to build trust with black box models. Anchors also gives the user the ability to predict the model behavior for unseen instances thereby being able to extract new insights from models built on top of complex data. 

However, the approach taken to achieve this task seems to be, in my personal opinion, unnecessarily complicated to accomodate a wide range of tasks. The reasoning behind such a stance is that exploring a finite set of possible bounding boxes in a high dimensional space while being computationally efficient should not require iterative solutions that do not guarantee optimal solutions. The usage of perturbation distributions has been a limitation in previous methods, as it restricts the method of generating samples to a particular distribution which might not be similar to the data generating distribution. 

## Discussion on reproducing approach

In terms of the attempt to simplify the implementation of anchors, the usage of sequentially greedy approach has proven to be quite useful while trying to use UCB as a balance between exploitation and exploration has proven to be detrimental.

## Conclusions

Overall, regardless of the semantics of the implementation, based on the results of the evaluation data, anchors has performed comparatively better than LIME, another popular explainable AI method. The simplified approach is a good initiative to explain the construction of anchors to educate and encourage researchers to use anchors in their modeling pipeline. Bridging the knowledge gap in complex tools can help users identify the reasoning behind the different techniques employed by the underlying tools that they use.

<!-- Simply put what I want this document to flow is to take them through the journey by first explaining 
what explainable AI is, and why it is important and what are the different variations of explainable AI

Then I will come in to talk about what anchors is and relate it to LIME and counterfactuals?

After that I will start off by fleshing my story on anchors. 

First formally define what an anchor is according to the authors. 
So to do that you will need to talk about what a predicate is and what a perturbation distribution is and what coverage and precision is. Then define the problem statement to finding anchors. and then explain how the authors have decided to find it. Give an overview of how you are going to construct anchors in an intuitive way by showcasing why the authors have decided to construct their solution in such a manner by demonstrating it for one dimensional, two dimensional data with two approaches. finally wrap up the document with an explaination of what the paper is lacking and what could be improved and where they have excelled.  -->
