---
title: "Explaining anchors"
format: html
---

This document shall be an explanation of the paper *Anchors: High-Precision Model-Agnostic Explanations*

Anchors are a model agnostic method of explaining black box models by presenting the model's decision process for a single given instance. The way anchors works is by generating a boundary box around the instance such that this box covers a large number of the data points that would receive similar model decisions as much as possible. 

## One dimensional example

Let's explore this idea with a simple one dimensional example.

```{r}
#| warning: false
#| message: false

library(tidyverse) # getting a sword to cut a sandwich
theme_set(theme_minimal())

# get the population x
pop_x <- seq(0,1,by = 0.01)
# assign a class
outcome <- ifelse(sin(15 * pop_x) > 0, "Plus", "Minus")
pop_data <- tibble(x = pop_x, class = factor(outcome))
# visualize population
ggplot(pop_data, aes(x = x,color = class, y =0)) + geom_point()
```

```{r}
# sample half of it
set.seed(100)
sample_data <- pop_data |> slice_sample(prop = 0.5, by = class)
ggplot(sample_data, aes(x = x,color = class, y =0)) + geom_point()
```

```{r}
# create a training and testing set
set.seed(200)
train_df <- sample_data |> slice_sample(prop = 0.7, by = class)
test_df <- sample_data |> slice_sample(prop = 0.3, by = class)
ggplot(train_df, aes(x = x,color = class, y =0)) + geom_point()
```

```{r}
# fit a randomforest model
library(randomForest)
rfmodel <- randomForest(class ~ x, data = train_df, ntree = 10)
rfmodel
```

Now how do we explain the decision process of this black box model using anchors. While it is possible to explain the decision process of a randomforest model with 10 trees fitted on a 1-dimensional data, we want to start explaining the process of anchors with a simpler case of one dimensional data.

## Constructing anchors

On the paper itself, anchors are defined as a rule or a set of predicates that satisfy the given instance and is a sufficient condition for $f(x)$ (i.e. the model output) with high probability.

An anchor is simply put a list of predicates. A predicate is a logical condition that an observation may or may not satisfy. The simplest form of these predicates takes in the form of $\{x_1 > 2\}$. An instance with the $x_1$ feature greater than 2 would satisfy this predicate. Therefore a possible candidate for an anchor might take the form of $A = \{x_1 > 2, x_1 < 3, x_2 > 10, x_3 < 5,x_4 == 1, \dots\}$. The paper itself does not define exactly how a predicate should be structured and therefore we will assume the simplest form of orthogonal boundaries.

For an list of predicates to be an anchor it should maximize two criteria,

1.  Precision
    1.  The precision is defined formally $Prec(A) = E_{D(z|A)}[1_{f(x) = f(z)}]$.
    2.  Here $D$ is the perturbation distribution based on the given instance $x$. A perturbation distribution is a method of generating varied versions of the data (kind of like alternate realities of the same data). The simplest form of a pertubation distribution would be to use a multivariate normal distribution centered around the given instance.
    3.  The paper argues that it is intractable to calculate the precision directly (the term directly could be meaning analytically as it is numerically possible to approximate the expected value)
    4.  Therefore a list of predicates $A$ is considered an anchor if $P(Prec(A) \ge \tau) \ge 1 - \delta$.
    5.  From an implementation perspective the precision of the anchor would then be the proportion of data points from the perturbation distribution with the same class as the given instance within the boundary of the anchor.
2.  Coverage
    1.  The paper defines the coverage of an anchor $A$ as the probability that it applies to samples from $D$, $cov(A) = E_{D(z)}[A(z)]$.
    2.  Simply put we would be calculating the proportion of samples from the perturbation distribution that satisfy the boundary of the anchor.
    3.  However, I would argue that picking a boundary that captures the most of the perturbation distribution is not ideal and instead it would be best to compute the coverage based on the proportion of the feature space that is covered.

In the paper the optimization problem is defined to maximize the coverage while ensuring that the precision is above a tolerance level.

## Implementing anchors

Below I have defined anchors using as a collection of predicates, while a predicate is defined as the collection of a feature name, a logical operator and a constant value to compare with. 

`anchors and predicate`
```{r}
library(S7)
predicate <- new_class("predicate", 
  properties = list(
    feature = class_character,
    operator = class_function,
    constant = new_union(
      class_integer, class_double, class_character
    )
  ),
  validator = function(self) {
  }
)

anchors <- new_class("anchors",
  properties = list(
    predicates = class_vector # a vector of predicate class
  ),
  validator = function(self) {
    if(!all(sapply(self@predicates, \(x) S7_inherits(x, predicate)))) {
      return("The list of predicates should all inherit from the predicate class ")
    }
  }
)
```


In addition, there are several other functions that I have defined that work on top of anchors

An anchor should be extendable by a predicate
`extend`
```{r}
#' @export
extend <- S7::new_generic("extend", "x")
#' @param pred The predicate to extend x with
#' @return Extended anchor
S7::method(extend, anchors) <- function(x, pred) {
  x@predicates <- c(x@predicates, pred)
  return(x)
}
```

Given a dataset it tells how many of the data points are satisfied through the boundary (i.e. how many data points are inside the boundary)

`satisfies`
```{r}
#' @export
satisfies <- S7::new_generic("satisfies", "x")
#' @param data The dataframe to apply anchors on. Can be one instance or an entire dataset.
#' @return A logical vector indicating whether the anchors satisfies `data`
S7::method(satisfies, anchors) <- function(x, data) {
  predicate_cols <- sapply(x@predicates, \(x) x@feature)
  if (!all(predicate_cols %in% colnames(data))) {
    stop(glue::glue(
      "Predicates contain the following columns \n {predicate_cols}\n",
      "that might not be in the dataset with the following columns \n {colnames(data)}"
    ))
  }
  satis_list <- rep(TRUE, nrow(data))
  for (predicate in x@predicates) {
    result_list <- predicate@operator(data[[predicate@feature]], predicate@constant)
    satis_list <- satis_list & result_list
  }
  return(satis_list)
}
```

The way precision is defined is by collecting samples from the perturbation distribution (i.e. the varied realities of the local instance) and then selecting the ones that are within the boundary to apply the model on top of those filtered points and calculating the proportion of class labels. 
`precision`
```{r}
precision <- S7::new_generic("precision", "x")
#' @param model a predict function that will provide the predicted labels given a dataset
#' @param dist the function that can be used to generate samples by providing an argument n. Should return a dataframe with proper column names.
#' @param n_samples the number of samples to generate from `dist` (the perturbation distribution)
#' @return named vector of proportions
S7::method(precision, anchors) <- function(x, model, dist, n_samples = 100) {
  samples <- dist(n = n_samples)
  satisfying_rows <- which(satisfies(x, samples), arr.ind = TRUE)
  if(length(satisfying_rows) == 0) {
    message("No satisfying rows found returning NULL")
    return(NULL)
  }
  samples <- samples |>
    dplyr::slice(satisfying_rows)
  preds <- model(samples)
  return(prop = as.vector(table(preds) / sum(table(preds))))
}
```

> TODO: Change the coverage calculation method to the feature space based method.

`coverage`
```{r}
#' @export
coverage <- S7::new_generic("coverage", "x")
#' @param dist the function that can be used to generate samples by providing an argument n. Should return a dataframe with proper column names.
#' @param n_samples the number of samples to generate from `dist` (the perturbation distribution)
S7::method(coverage, anchors) <- function(x, dist, n_samples = 100) {
  samples <- dist(n = n_samples)
  return(mean(satisfies(x, samples)))
}
```

Let us first pick a data point in the dataset. Ideally a point in a border would be best to illustrate the idea. Hence we will pick the first observation as follows.

```{r}
local_instance <- 1
ggplot(train_df[-local_instance, ], aes(x = x,color = class, y = 0)) + geom_point() + geom_point(data = train_df[local_instance, ], color = "black", size = 2.5) +
  geom_label(data = train_df[local_instance,], label = "here", color = "black", nudge_y = 0.005) +
  lims(y = c(-0.05, 0.05))
```

For a one dimensional example, a boundary region would be defined by two values, a value left to the given value and a value right to the given value.

```{r}
x_vals <- train_df[-local_instance,][["x"]] |> sort()
x_cutpoints <- purrr::map2_dbl(x_vals[-length(x_vals)], x_vals[-1], function(x, x_1) {
  return(mean(c(x, x_1)))
})
x_grid <- expand.grid(
  x_cutpoints[x_cutpoints < train_df[local_instance,]$x],
  x_cutpoints[x_cutpoints > train_df[local_instance,]$x]
) |> rename(a = Var1, b = Var2)
```

All possible cutpoints

```{r}
ggplot(train_df[-local_instance, ], aes(x = x,color = class, y = 0)) + geom_point() + geom_point(data = train_df[local_instance, ], color = "black", size = 2.5) +
  geom_label(data = train_df[local_instance,], label = "here", color = "black", nudge_y = 0.005) +
  lims(y = c(-0.05, 0.05)) +
  geom_vline(data = x_grid, aes(xintercept = a), color = "purple", linetype = "dashed", alpha = 0.4) +
  geom_vline(data = x_grid, aes(xintercept = b), color = "gray", linetype = "dashed", alpha = 0.8)
```

Sample 3 bounding boxes

```{r}
ggplot(train_df[-local_instance, ], aes(x = x, y = 0)) + geom_point() + geom_point(data = train_df[local_instance, ], color = "black", size = 2.5) +
  geom_label(data = train_df[local_instance,], label = "here", color = "black", nudge_y = 0.005) +
  lims(y = c(-0.05, 0.05)) +
  geom_rect(
    data = x_grid |>
      slice_sample(n = 3) |>
      mutate(id = row_number()),
    aes(xmin = a, xmax = b, ymin = -0.05,ymax = 0.05, color = factor(id)),
    linetype = "dashed",
    fill = "transparent",
    inherit.aes = FALSE
  )
```

For all of the possible boundaries let's calculate the precision and accuracy

```{r}
model_func <- function(data_samples) {
  return(predict(rfmodel, data_samples))
}

dist_func <- function(n) data.frame(x = seq(0,1,by = 0.01))
```


```{r}
res <- x_grid |> apply(1, function(row) {
  bound <- anchors(c(
    predicate(feature = "x",operator = `>`,constant = row["a"]),
    predicate(feature = "x",operator = `<`,constant = row["b"])
  ))
  cover <- coverage(bound, dist_func, n_samples = 500)
  prec <- precision(bound, model_func, dist_func, n_samples = 500)
  return(list(cover = cover, prec = prec))
})
```


```{r}
res_df <- res |> map_dfr(~ tibble(cover = .x$cover, prec_1 = .x$prec[1], prec_2 = .x$prec[2]))
res_df |> ggplot(aes(x = prec_1, y = cover))+geom_point() + geom_vline(xintercept = 0.8)
```

Let us visualize the bounding box with the highest precision.

```{r}
max_prec_bound <- bind_cols(x_grid, res_df) |> slice_max(prec_1)
  ggplot(train_df[-local_instance, ], aes(x = x,color = class, y = 0)) + geom_point() + geom_point(data = train_df[local_instance, ], color = "black", size = 2.5) +
  geom_label(data = train_df[local_instance,], label = "here", color = "black", nudge_y = 0.005) +
  lims(y = c(-0.05, 0.05)) +
  geom_rect(
    data = max_prec_bound,
    aes(xmin = a, xmax = b, ymin = -0.05,ymax = 0.05),
    linetype = "dashed",
    color = "purple",
    fill = "transparent",
    inherit.aes = FALSE,
    show.legend = FALSE
  ) + labs(title = glue::glue("Precision: {round(max_prec_bound$prec_1,2)}", " Coverage = {round(max_prec_bound$cover,2)}"), subtitle = glue::glue("IF x > {max_prec_bound$a} AND x < {max_prec_bound$b}"), caption = "Plotted points are training data")
```

```{r}
ggplot(pop_data, aes(x = x,color = class, y = 0)) + geom_point() + geom_point(data = train_df[local_instance, ], color = "black", size = 2.5) +
  geom_label(data = train_df[local_instance,], label = "here", color = "black", nudge_y = 0.005) +
  lims(y = c(-0.05, 0.05)) +
  geom_rect(
    data = max_prec_bound,
    aes(xmin = a, xmax = b, ymin = -0.05,ymax = 0.05),
    linetype = "dashed",
    color = "purple",
    fill = "transparent",
    inherit.aes = FALSE,
    show.legend = FALSE
  ) +
  labs(title = glue::glue("Precision: {round(max_prec_bound$prec_1,2)}", " Coverage = {round(max_prec_bound$cover,2)}"), subtitle = glue::glue("IF x > {max_prec_bound$a} AND x < {max_prec_bound$b}"), caption = "Plotted points are population points")
```

The most optimal bounding box

```{r}
optimal_bound <- bind_cols(x_grid, res_df) |>
  arrange(desc(cover), desc(prec_1)) |>
  filter(prec_1 > 0.8) |> 
  slice(1)

ggplot(train_df[-local_instance, ], aes(x = x,color = class, y = 0)) + geom_point() + geom_point(data = train_df[local_instance, ], color = "black", size = 2.5) +
  geom_label(data = train_df[local_instance,], label = "here", color = "black", nudge_y = 0.005) +
  lims(y = c(-0.05, 0.05)) +
  geom_rect(
    data = optimal_bound,
    aes(xmin = a, xmax = b, ymin = -0.05,ymax = 0.05),
    linetype = "dashed",
    color = "purple",
    fill = "transparent",
    inherit.aes = FALSE,
    show.legend = FALSE
  ) +
  labs(title = glue::glue("Precision: {round(optimal_bound$prec_1,2)}", " Coverage = {round(optimal_bound$cover,2)}"), subtitle = glue::glue("IF x > {optimal_bound$a} AND x < {optimal_bound$b}"), caption = "Plotted points are training data points")
```

```{r}
ggplot(pop_data, aes(x = x,color = class, y = 0)) + geom_point() + geom_point(data = train_df[local_instance, ], color = "black", size = 2.5) +
  geom_label(data = train_df[local_instance,], label = "here", color = "black", nudge_y = 0.005) +
  lims(y = c(-0.05, 0.05)) +
  geom_rect(
    data = optimal_bound,
    aes(xmin = a, xmax = b, ymin = -0.05,ymax = 0.05),
    linetype = "dashed",
    color = "purple",
    fill = "transparent",
    inherit.aes = FALSE,
    show.legend = FALSE
  ) +
  labs(title = glue::glue("Precision: {round(optimal_bound$prec_1,2)}", " Coverage = {round(optimal_bound$cover,2)}"), subtitle = glue::glue("IF x > {optimal_bound$a} AND x < {optimal_bound$b}"), caption = "Plotted points are population points")
```


Now let's try to perform this for two dimensions.

```{r}
w <- read_csv("wiggly.csv") |> mutate(class = factor(ifelse(class == 3, "Positive", "Negative"))) |> select(-`...1`)
w |> ggplot(aes(x = x,y = y,color = class)) + geom_point()
```


```{r}
# sample train data
set.seed(69420)
train_indices <- sample(nrow(w), round(nrow(w) * 0.8))
w[train_indices, ] |> ggplot(aes(x = x,y = y,color = class)) + geom_point()
```



```{r}
train_df <- w[train_indices, ] |> mutate(id = row_number())
library(randomForest)
rfmodel <- randomForest(class ~ x + y, data = train_df, ntree = 5)
rfmodel
```



```{r}
# select instance
prob_matrix <- predict(rfmodel, newdata = train_df, type = "prob") |> 
  as.data.frame() |>
  mutate(id = row_number())
local_instance <- prob_matrix[prob_matrix$Negative == 0.4, "id"]
print(local_instance)
local_instance <- local_instance[1]
```


```{r}
train_df[-local_instance, ] |>
  ggplot(aes(x = x,y = y,color = class)) +
  geom_point() +
  geom_point(data = train_df[local_instance, ], size = 5, color = "black") +
  geom_label(data = train_df[local_instance, ], label = "here", color = "black", nudge_y = 0.05)
```

```{r}
# generate cutpoints
x_vals <- train_df[-local_instance,][["x"]] |> sort()
x_cutpoints <- purrr::map2_dbl(x_vals[-length(x_vals)], x_vals[-1], function(x, x_1) {
  return(mean(c(x, x_1)))
})
x_grid <- expand.grid(
  x_cutpoints[x_cutpoints < train_df[local_instance,]$x],
  x_cutpoints[x_cutpoints > train_df[local_instance,]$x]
) |> 
  rename(L = Var1, U = Var2)|> 
  as_tibble() |>
  arrange(desc(L), U)

y_vals <- train_df[-local_instance,][["y"]] |> sort()
y_cutpoints <- purrr::map2_dbl(y_vals[-length(y_vals)], y_vals[-1], function(x, x_1) {
  return(mean(c(x, x_1)))
})
y_grid <- expand.grid(
  y_cutpoints[y_cutpoints < train_df[local_instance,]$y],
  y_cutpoints[y_cutpoints > train_df[local_instance,]$y]
) |> 
  rename(L = Var1, U = Var2) |> 
  as_tibble() |>
  arrange(desc(L), U)
```

```{r}
pertub_func <- function(n) {
  mulgar::rmvn(n = n, 
               p = 2,
               mn = train_df[local_instance, c("x", "y")] |> 
                 unlist(),
               vc = cov(train_df[,c("x", "y")])
  ) |>
    as.data.frame() |>
    rename(x = x1, y = x2)
}

model_func <- function(data_samples) {
  return(predict(rfmodel, data_samples))
}

set.seed(123)
samples <- pertub_func(n = 10000)
dist_func <- function(n) samples[1:n, ]
```

We are going to brute force the two dimensional approach sequentially


```{r}
# define final anchor to be null
final_anchor <- NULL
dimensions <- list("x" = x_grid,"y" = y_grid) # variable names as names
results <- imap(dimensions, function(bounds, var_name){
  dim_results <- map_dfr(seq_len(nrow(bounds)), function(i) {
    row <- bounds[i, ]
    lower_bound_pred <- predicate(feature = var_name, operator = `>`, constant = row[["L"]])
    upper_bound_pred <- predicate(feature = var_name, operator = `<`, constant = row[["U"]])
    if(is.null(final_anchor)) {
      bound <- anchors(c(lower_bound_pred, upper_bound_pred))  
    } else {
      bound <- final_anchor |>
        extend(lower_bound_pred) |>
        extend(upper_bound_pred)
    }
    cover <- coverage(bound, dist_func, n_samples = 10000)
    prec <- precision(bound, model_func, dist_func, n_samples = 10000)
    bind_cols(row, tibble(cover = cover, precision = prec))
  })
  max_prec <- dim_results |> slice_max(precision) |> head(1)
  best_lower_bound <- predicate(feature = var_name, operator = `>`, constant = max_prec[["L"]])
  best_upper_bound <- predicate(feature = var_name, operator = `<`, constant = max_prec[["U"]])
  if(is.null(final_anchor)) {
    final_anchor <<- anchors(c(best_lower_bound, best_upper_bound))  
  } else {
    final_anchor <<- final_anchor |>
        extend(best_lower_bound) |>
        extend(best_upper_bound)
  }
  list(res = dim_results, lb = best_lower_bound, ub = best_upper_bound)
})
```


```{r}
train_df[-local_instance, ] |>
  ggplot(aes(x = x,y = y,color = class)) +
  geom_point() +
  geom_point(data = train_df[local_instance, ], size = 1, color = "black") +
  geom_label(data = train_df[local_instance, ], label = "here", color = "black", nudge_y = 0.05) +
  geom_rect(inherit.aes = F, 
            data = tibble(x_lb = results$x$lb@constant,
                          y_lb = results$y$lb@constant,
                          x_ub = results$x$ub@constant,
                          y_ub = results$y$ub@constant),
            aes(xmin = x_lb, xmax = x_ub, ymin = y_lb, ymax = y_ub), fill = "transparent", color = "black")
```

The sequential greedy method does seem to be generating good boundaries.
Let us try modeling this as a multiarmed bandit problem.

the actions are to increase the lower / upper bound of each dimension.

```{r}
# TODO we can actually model the actions as changing the bounding box in a given direction
envir <- list(
  x_lb = x_grid$L |> unique(),
  x_ub = x_grid$U |> unique(),
  y_lb = y_grid$L |> unique(),
  y_ub = y_grid$U |> unique()
)
actions <- c("x_lb", "x_ub", "y_lb", "y_ub")
```

```{r}
get_reward <- function(x_lb_ind, x_ub_ind, y_lb_ind, y_ub_ind, dist_func, model_func, class_ind = 1) {
  bound <- anchors(c(
    predicate(feature = "x",operator = `>`,constant = envir$x_lb[x_lb_ind]),
    predicate(feature = "x",operator = `<`,constant = envir$x_ub[x_ub_ind]),
    predicate(feature = "y",operator = `>`,constant = envir$y_lb[y_lb_ind]),
    predicate(feature = "y",operator = `<`,constant = envir$y_ub[y_ub_ind])
  ))
  # if(!satisfies(bound, train_df[local_instance, ])) {
  #   print(c(x_lb_ind, x_ub_ind, y_lb_ind, y_ub_ind))
  #   return(-9999) # penalty
  # }
  cover <- coverage(bound, dist_func, n_samples = 10000)
  prec <- precision(bound, model_func, dist_func, n_samples = 10000)
  if(is.null(prec)) return(-9999) # penalty
  if(prec[class_ind] < 0.6) {
    return(-9999) # penalty for wrong direction
  } 
  # return(2 * prec[class_ind] + 0.5 * cover) # to put more weight on precision
  return(prec[class_ind])
}

select_action <- function(Q, N, n_game) {
  rewards <- map_dbl(actions, function(a){
    Q[[a]] + sqrt((2 * log(n_game)) / N[[a]])
  })
  if(sum(rewards == max(rewards)) > 1) {
    max_reward <- sample(which(rewards == max(rewards)), 1)
  } else {
    max_reward <- which.max(rewards)  
  }
  return(actions[max_reward])
}
```

```{r}
n_games <- 10
n_epochs <- 100
```

```{r}
for(game in seq_len(n_games)) {
  x_ub_ind <- 1
  x_lb_ind <- 1
  y_ub_ind <- 1
  y_lb_ind <- 1
  N <- list(x_ub = 1, y_ub = 1, x_lb = 1, y_lb = 1)
  Q <- list(x_ub = 0, y_ub = 0, x_lb = 0, y_lb = 0)
  for(epoch in seq_len(n_epochs)) {
    action <- select_action(Q, N, game)
    if(action == "x_ub") x_ub_ind <- ifelse(x_ub_ind == length(envir$x_ub), x_ub_ind, x_ub_ind + 1)
    if(action == "y_ub") y_ub_ind <- ifelse(y_ub_ind == length(envir$y_ub), y_ub_ind, y_ub_ind + 1)
    if(action == "x_lb") x_lb_ind <- ifelse(x_lb_ind == length(envir$x_lb), x_lb_ind, x_lb_ind + 1)
    if(action == "y_lb") y_lb_ind <- ifelse(y_lb_ind == length(envir$y_lb), y_lb_ind, y_lb_ind + 1)
    reward <- get_reward(x_lb_ind, x_ub_ind, y_lb_ind, y_ub_ind, dist_func, model_func)
    if(reward < 0) {
      # if a penalty was received we undo the action and go on with the rest
      if(action == "x_ub") x_ub_ind <- ifelse(x_ub_ind == 1, x_ub_ind, x_ub_ind - 1)
      if(action == "y_ub") y_ub_ind <- ifelse(y_ub_ind == 1, y_ub_ind, y_ub_ind - 1)
      if(action == "x_lb") x_lb_ind <- ifelse(x_lb_ind == 1, x_lb_ind, x_lb_ind - 1)
      if(action == "y_lb") y_lb_ind <- ifelse(y_lb_ind == 1, y_lb_ind, y_lb_ind - 1)
      next
    }
    N[[action]] <- N[[action]] + 1
    Q[[action]] <- Q[[action]] + ((reward - Q[[action]]) / N[[action]])
    if(epoch %% 10 == 0) {
      state_plot <- train_df[-local_instance, ] |>
        ggplot(aes(x = x,y = y,color = class)) +
        geom_point() +
        geom_point(data = train_df[local_instance, ], size = 1, color = "black") +
        geom_label(data = train_df[local_instance, ], label = "here", color = "black", nudge_y = 0.05) +
        geom_rect(inherit.aes = F,
                  data = tibble(x_lb = envir$x_lb[x_lb_ind],
                                y_lb = envir$y_lb[y_lb_ind],
                                x_ub = envir$x_ub[x_ub_ind],
                                y_ub = envir$y_ub[y_ub_ind]),
                  aes(xmin = x_lb, xmax = x_ub, ymin = y_lb, ymax = y_ub), fill = "transparent", color = "black") +
        labs(title = glue::glue("Game: {game}, round: {epoch}, reward: {reward}"))
      ggsave(plot = state_plot,
             filename = here::here("scratchpad/3_state_plot_dump/", glue::glue("{game}_{epoch}.png")),
             device = "png", bg = "white", width = 11, height = 8, units = "in")
    }
  }
}
```

Let's go into even higher dimensions.

```{r}
library(mulgar)
library(geozoo)
library(tourr)

set.seed(1071)
geozoo::torus() -> tor
tor_points <- tor$points |> as_tibble()
torus_points <- tor$points |> apply(1, \(x) ifelse(sum(sin(x)) > 0.5, "red", "orange"))
# render_gif()
# anim <- animate(data = tor_points, display = display_xy(col = torus_points), position = "center")
```

```{r}
set.seed(69420)
tor_data <- tor_points |> mutate(class = torus_points) |> rename(x = V1, y = V2, z = V3)
train_indices <- sample(nrow(tor_data), round(nrow(tor_data) * 0.8))
train_df <- tor_data[train_indices, ]
```


```{r}
source(here::here("R/anchors.R"))
```
